{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:16:27.778763Z",
     "start_time": "2024-04-24T16:16:24.465487Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.data import Data"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "4ca648e1653f6bc2",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "id": "703cc2509b28b888",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:16:27.841642Z",
     "start_time": "2024-04-24T16:16:27.778763Z"
    }
   },
   "source": [
    "bp_db = pd.read_csv(\"../data/bp_db.csv\", index_col=0)\n",
    "bp_db.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           ENSEMBL          GO\n",
       "0  ENSG00000000003  GO:0039532\n",
       "1  ENSG00000000005  GO:0001937\n",
       "2  ENSG00000000005  GO:0016525\n",
       "3  ENSG00000000419  GO:0006488\n",
       "4  ENSG00000000419  GO:0006506"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSEMBL</th>\n",
       "      <th>GO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000000003</td>\n",
       "      <td>GO:0039532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000000005</td>\n",
       "      <td>GO:0001937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000000005</td>\n",
       "      <td>GO:0016525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000000419</td>\n",
       "      <td>GO:0006488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000000419</td>\n",
       "      <td>GO:0006506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "de83495d06a06d39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:16:34.373974Z",
     "start_time": "2024-04-24T16:16:27.841642Z"
    }
   },
   "source": [
    "counts1 = pd.read_csv(\"../data/counts1.csv\", index_col=0)\n",
    "counts1.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         ENSG00000000003  ENSG00000000005  ENSG00000000419  ENSG00000000457  \\\n",
       "089357B               14                7              103              241   \n",
       "089366A               11                2              194              511   \n",
       "089412B                8                0              312              450   \n",
       "089425B                9                0              135              496   \n",
       "089687A                4                0               89              267   \n",
       "\n",
       "         ENSG00000000460  ENSG00000000938  ENSG00000000971  ENSG00000001036  \\\n",
       "089357B               72             2057               30               60   \n",
       "089366A              110             3325               36              111   \n",
       "089412B              106             3751               45              160   \n",
       "089425B              133             2758               26               93   \n",
       "089687A               49             2181               24               75   \n",
       "\n",
       "         ENSG00000001084  ENSG00000001167  ...  ENSGR0000167393  \\\n",
       "089357B              207              367  ...                1   \n",
       "089366A              186              530  ...                0   \n",
       "089412B              325              653  ...                0   \n",
       "089425B              182              620  ...                0   \n",
       "089687A              122              263  ...                0   \n",
       "\n",
       "         ENSGR0000169084  ENSGR0000169093  ENSGR0000178605  ENSGR0000182378  \\\n",
       "089357B                0                0                0                0   \n",
       "089366A                0                0                0                0   \n",
       "089412B                0                0                0                0   \n",
       "089425B                0                0                0                0   \n",
       "089687A                0                0                0                0   \n",
       "\n",
       "         ENSGR0000185291  ENSGR0000198223  ENSGR0000214717  ENSGR0000223511  \\\n",
       "089357B                0                0                0                0   \n",
       "089366A                0                1                0                0   \n",
       "089412B                0                1                0                0   \n",
       "089425B                0                0                0                0   \n",
       "089687A                0                1                0                0   \n",
       "\n",
       "         ENSGR0000223773  \n",
       "089357B                0  \n",
       "089366A                1  \n",
       "089412B                0  \n",
       "089425B                0  \n",
       "089687A                0  \n",
       "\n",
       "[5 rows x 52645 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000000003</th>\n",
       "      <th>ENSG00000000005</th>\n",
       "      <th>ENSG00000000419</th>\n",
       "      <th>ENSG00000000457</th>\n",
       "      <th>ENSG00000000460</th>\n",
       "      <th>ENSG00000000938</th>\n",
       "      <th>ENSG00000000971</th>\n",
       "      <th>ENSG00000001036</th>\n",
       "      <th>ENSG00000001084</th>\n",
       "      <th>ENSG00000001167</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSGR0000167393</th>\n",
       "      <th>ENSGR0000169084</th>\n",
       "      <th>ENSGR0000169093</th>\n",
       "      <th>ENSGR0000178605</th>\n",
       "      <th>ENSGR0000182378</th>\n",
       "      <th>ENSGR0000185291</th>\n",
       "      <th>ENSGR0000198223</th>\n",
       "      <th>ENSGR0000214717</th>\n",
       "      <th>ENSGR0000223511</th>\n",
       "      <th>ENSGR0000223773</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>089357B</th>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>103</td>\n",
       "      <td>241</td>\n",
       "      <td>72</td>\n",
       "      <td>2057</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>207</td>\n",
       "      <td>367</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089366A</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>194</td>\n",
       "      <td>511</td>\n",
       "      <td>110</td>\n",
       "      <td>3325</td>\n",
       "      <td>36</td>\n",
       "      <td>111</td>\n",
       "      <td>186</td>\n",
       "      <td>530</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089412B</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>450</td>\n",
       "      <td>106</td>\n",
       "      <td>3751</td>\n",
       "      <td>45</td>\n",
       "      <td>160</td>\n",
       "      <td>325</td>\n",
       "      <td>653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089425B</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>496</td>\n",
       "      <td>133</td>\n",
       "      <td>2758</td>\n",
       "      <td>26</td>\n",
       "      <td>93</td>\n",
       "      <td>182</td>\n",
       "      <td>620</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089687A</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>267</td>\n",
       "      <td>49</td>\n",
       "      <td>2181</td>\n",
       "      <td>24</td>\n",
       "      <td>75</td>\n",
       "      <td>122</td>\n",
       "      <td>263</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 52645 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "8f679c3537ff4d32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:16:34.389588Z",
     "start_time": "2024-04-24T16:16:34.373974Z"
    }
   },
   "source": [
    "pheno1 = pd.read_csv(\"../data/pheno1.csv\", index_col=0)\n",
    "pheno1.drop([\"diagnosis\"], axis=1, inplace=True)\n",
    "pheno1[\"condition\"] = pheno1[\"condition\"].apply(lambda x: 0 if x == \"Control\" else 1)\n",
    "pheno1.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         age sex  lithium  condition\n",
       "089357B   18   F        0          0\n",
       "089366A   19   F        0          0\n",
       "089412B   23   F        0          0\n",
       "089425B   47   F        0          0\n",
       "089687A   52   F        0          0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>lithium</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>089357B</th>\n",
       "      <td>18</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089366A</th>\n",
       "      <td>19</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089412B</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089425B</th>\n",
       "      <td>47</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089687A</th>\n",
       "      <td>52</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "6dccdac6a31a835f",
   "metadata": {},
   "source": [
    "# Train Node Embeddings from Graph"
   ]
  },
  {
   "cell_type": "code",
   "id": "dfc2317bcd4658fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:16:35.390041Z",
     "start_time": "2024-04-24T16:16:34.389588Z"
    }
   },
   "source": [
    "bp_graph = nx.read_gml(\"../data/bp_graph.gml\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "1e4c6f44154cac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:16:35.499697Z",
     "start_time": "2024-04-24T16:16:35.390041Z"
    }
   },
   "source": [
    "bp_db_go = sorted(set(bp_graph.nodes))\n",
    "\n",
    "map_int_go = {int(idx): go for idx, go in enumerate(bp_db_go)}\n",
    "map_go_int = {go: idx for idx, go in map_int_go.items()}\n",
    "\n",
    "_graph = nx.relabel_nodes(bp_graph, map_go_int, copy=False)\n",
    "edge_index = torch.tensor(list(_graph.edges()), dtype=torch.long).t().contiguous()\n",
    "\n",
    "data = Data(x=None, edge_index=edge_index)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "6d464d014f28b6b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:16:35.671572Z",
     "start_time": "2024-04-24T16:16:35.499697Z"
    }
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Node2Vec(data.edge_index, embedding_dim=8, walk_length=20,\n",
    "                 context_size=10, walks_per_node=10,\n",
    "                 num_negative_samples=1, p=1, q=1, sparse=True).to(device)\n",
    "\n",
    "loader = model.loader(batch_size=4, shuffle=True)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.001)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "f8c00d52b2dcc4a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:16:35.687197Z",
     "start_time": "2024-04-24T16:16:35.671572Z"
    }
   },
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "c53c639ee3271d26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:06:55.280008Z",
     "start_time": "2024-04-24T16:16:47.964752Z"
    }
   },
   "source": [
    "losses = []\n",
    "for epoch in range(1, 500):\n",
    "    loss = train()\n",
    "    losses.append(loss)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.3342\n",
      "Epoch: 02, Loss: 1.1313\n",
      "Epoch: 03, Loss: 1.0369\n",
      "Epoch: 04, Loss: 0.9800\n",
      "Epoch: 05, Loss: 0.9409\n",
      "Epoch: 06, Loss: 0.9127\n",
      "Epoch: 07, Loss: 0.8917\n",
      "Epoch: 08, Loss: 0.8752\n",
      "Epoch: 09, Loss: 0.8615\n",
      "Epoch: 10, Loss: 0.8500\n",
      "Epoch: 11, Loss: 0.8406\n",
      "Epoch: 12, Loss: 0.8326\n",
      "Epoch: 13, Loss: 0.8255\n",
      "Epoch: 14, Loss: 0.8198\n",
      "Epoch: 15, Loss: 0.8141\n",
      "Epoch: 16, Loss: 0.8095\n",
      "Epoch: 17, Loss: 0.8054\n",
      "Epoch: 18, Loss: 0.8014\n",
      "Epoch: 19, Loss: 0.7980\n",
      "Epoch: 20, Loss: 0.7952\n",
      "Epoch: 21, Loss: 0.7923\n",
      "Epoch: 22, Loss: 0.7897\n",
      "Epoch: 23, Loss: 0.7876\n",
      "Epoch: 24, Loss: 0.7855\n",
      "Epoch: 25, Loss: 0.7837\n",
      "Epoch: 26, Loss: 0.7821\n",
      "Epoch: 27, Loss: 0.7807\n",
      "Epoch: 28, Loss: 0.7792\n",
      "Epoch: 29, Loss: 0.7777\n",
      "Epoch: 30, Loss: 0.7765\n",
      "Epoch: 31, Loss: 0.7756\n",
      "Epoch: 32, Loss: 0.7745\n",
      "Epoch: 33, Loss: 0.7738\n",
      "Epoch: 34, Loss: 0.7728\n",
      "Epoch: 35, Loss: 0.7720\n",
      "Epoch: 36, Loss: 0.7713\n",
      "Epoch: 37, Loss: 0.7706\n",
      "Epoch: 38, Loss: 0.7703\n",
      "Epoch: 39, Loss: 0.7695\n",
      "Epoch: 40, Loss: 0.7691\n",
      "Epoch: 41, Loss: 0.7688\n",
      "Epoch: 42, Loss: 0.7683\n",
      "Epoch: 43, Loss: 0.7678\n",
      "Epoch: 44, Loss: 0.7675\n",
      "Epoch: 45, Loss: 0.7672\n",
      "Epoch: 46, Loss: 0.7668\n",
      "Epoch: 47, Loss: 0.7665\n",
      "Epoch: 48, Loss: 0.7663\n",
      "Epoch: 49, Loss: 0.7659\n",
      "Epoch: 50, Loss: 0.7657\n",
      "Epoch: 51, Loss: 0.7655\n",
      "Epoch: 52, Loss: 0.7651\n",
      "Epoch: 53, Loss: 0.7649\n",
      "Epoch: 54, Loss: 0.7649\n",
      "Epoch: 55, Loss: 0.7647\n",
      "Epoch: 56, Loss: 0.7644\n",
      "Epoch: 57, Loss: 0.7645\n",
      "Epoch: 58, Loss: 0.7643\n",
      "Epoch: 59, Loss: 0.7641\n",
      "Epoch: 60, Loss: 0.7640\n",
      "Epoch: 61, Loss: 0.7639\n",
      "Epoch: 62, Loss: 0.7641\n",
      "Epoch: 63, Loss: 0.7635\n",
      "Epoch: 64, Loss: 0.7635\n",
      "Epoch: 65, Loss: 0.7633\n",
      "Epoch: 66, Loss: 0.7634\n",
      "Epoch: 67, Loss: 0.7633\n",
      "Epoch: 68, Loss: 0.7631\n",
      "Epoch: 69, Loss: 0.7629\n",
      "Epoch: 70, Loss: 0.7630\n",
      "Epoch: 71, Loss: 0.7628\n",
      "Epoch: 72, Loss: 0.7628\n",
      "Epoch: 73, Loss: 0.7630\n",
      "Epoch: 74, Loss: 0.7629\n",
      "Epoch: 75, Loss: 0.7626\n",
      "Epoch: 76, Loss: 0.7625\n",
      "Epoch: 77, Loss: 0.7625\n",
      "Epoch: 78, Loss: 0.7622\n",
      "Epoch: 79, Loss: 0.7623\n",
      "Epoch: 80, Loss: 0.7622\n",
      "Epoch: 81, Loss: 0.7625\n",
      "Epoch: 82, Loss: 0.7622\n",
      "Epoch: 83, Loss: 0.7623\n",
      "Epoch: 84, Loss: 0.7620\n",
      "Epoch: 85, Loss: 0.7619\n",
      "Epoch: 86, Loss: 0.7620\n",
      "Epoch: 87, Loss: 0.7620\n",
      "Epoch: 88, Loss: 0.7619\n",
      "Epoch: 89, Loss: 0.7617\n",
      "Epoch: 90, Loss: 0.7617\n",
      "Epoch: 91, Loss: 0.7618\n",
      "Epoch: 92, Loss: 0.7616\n",
      "Epoch: 93, Loss: 0.7616\n",
      "Epoch: 94, Loss: 0.7614\n",
      "Epoch: 95, Loss: 0.7613\n",
      "Epoch: 96, Loss: 0.7614\n",
      "Epoch: 97, Loss: 0.7612\n",
      "Epoch: 98, Loss: 0.7611\n",
      "Epoch: 99, Loss: 0.7612\n",
      "Epoch: 100, Loss: 0.7613\n",
      "Epoch: 101, Loss: 0.7614\n",
      "Epoch: 102, Loss: 0.7611\n",
      "Epoch: 103, Loss: 0.7609\n",
      "Epoch: 104, Loss: 0.7610\n",
      "Epoch: 105, Loss: 0.7609\n",
      "Epoch: 106, Loss: 0.7610\n",
      "Epoch: 107, Loss: 0.7610\n",
      "Epoch: 108, Loss: 0.7607\n",
      "Epoch: 109, Loss: 0.7609\n",
      "Epoch: 110, Loss: 0.7608\n",
      "Epoch: 111, Loss: 0.7608\n",
      "Epoch: 112, Loss: 0.7607\n",
      "Epoch: 113, Loss: 0.7606\n",
      "Epoch: 114, Loss: 0.7608\n",
      "Epoch: 115, Loss: 0.7607\n",
      "Epoch: 116, Loss: 0.7605\n",
      "Epoch: 117, Loss: 0.7604\n",
      "Epoch: 118, Loss: 0.7607\n",
      "Epoch: 119, Loss: 0.7605\n",
      "Epoch: 120, Loss: 0.7603\n",
      "Epoch: 121, Loss: 0.7603\n",
      "Epoch: 122, Loss: 0.7601\n",
      "Epoch: 123, Loss: 0.7603\n",
      "Epoch: 124, Loss: 0.7602\n",
      "Epoch: 125, Loss: 0.7604\n",
      "Epoch: 126, Loss: 0.7604\n",
      "Epoch: 127, Loss: 0.7603\n",
      "Epoch: 128, Loss: 0.7603\n",
      "Epoch: 129, Loss: 0.7602\n",
      "Epoch: 130, Loss: 0.7601\n",
      "Epoch: 131, Loss: 0.7601\n",
      "Epoch: 132, Loss: 0.7601\n",
      "Epoch: 133, Loss: 0.7599\n",
      "Epoch: 134, Loss: 0.7601\n",
      "Epoch: 135, Loss: 0.7598\n",
      "Epoch: 136, Loss: 0.7598\n",
      "Epoch: 137, Loss: 0.7598\n",
      "Epoch: 138, Loss: 0.7599\n",
      "Epoch: 139, Loss: 0.7597\n",
      "Epoch: 140, Loss: 0.7598\n",
      "Epoch: 141, Loss: 0.7598\n",
      "Epoch: 142, Loss: 0.7596\n",
      "Epoch: 143, Loss: 0.7598\n",
      "Epoch: 144, Loss: 0.7595\n",
      "Epoch: 145, Loss: 0.7597\n",
      "Epoch: 146, Loss: 0.7595\n",
      "Epoch: 147, Loss: 0.7596\n",
      "Epoch: 148, Loss: 0.7595\n",
      "Epoch: 149, Loss: 0.7594\n",
      "Epoch: 150, Loss: 0.7594\n",
      "Epoch: 151, Loss: 0.7596\n",
      "Epoch: 152, Loss: 0.7595\n",
      "Epoch: 153, Loss: 0.7595\n",
      "Epoch: 154, Loss: 0.7594\n",
      "Epoch: 155, Loss: 0.7594\n",
      "Epoch: 156, Loss: 0.7595\n",
      "Epoch: 157, Loss: 0.7593\n",
      "Epoch: 158, Loss: 0.7595\n",
      "Epoch: 159, Loss: 0.7590\n",
      "Epoch: 160, Loss: 0.7595\n",
      "Epoch: 161, Loss: 0.7591\n",
      "Epoch: 162, Loss: 0.7591\n",
      "Epoch: 163, Loss: 0.7593\n",
      "Epoch: 164, Loss: 0.7593\n",
      "Epoch: 165, Loss: 0.7592\n",
      "Epoch: 166, Loss: 0.7592\n",
      "Epoch: 167, Loss: 0.7593\n",
      "Epoch: 168, Loss: 0.7592\n",
      "Epoch: 169, Loss: 0.7592\n",
      "Epoch: 170, Loss: 0.7589\n",
      "Epoch: 171, Loss: 0.7591\n",
      "Epoch: 172, Loss: 0.7590\n",
      "Epoch: 173, Loss: 0.7588\n",
      "Epoch: 174, Loss: 0.7589\n",
      "Epoch: 175, Loss: 0.7589\n",
      "Epoch: 176, Loss: 0.7592\n",
      "Epoch: 177, Loss: 0.7591\n",
      "Epoch: 178, Loss: 0.7587\n",
      "Epoch: 179, Loss: 0.7590\n",
      "Epoch: 180, Loss: 0.7589\n",
      "Epoch: 181, Loss: 0.7589\n",
      "Epoch: 182, Loss: 0.7589\n",
      "Epoch: 183, Loss: 0.7588\n",
      "Epoch: 184, Loss: 0.7589\n",
      "Epoch: 185, Loss: 0.7588\n",
      "Epoch: 186, Loss: 0.7589\n",
      "Epoch: 187, Loss: 0.7588\n",
      "Epoch: 188, Loss: 0.7586\n",
      "Epoch: 189, Loss: 0.7590\n",
      "Epoch: 190, Loss: 0.7588\n",
      "Epoch: 191, Loss: 0.7588\n",
      "Epoch: 192, Loss: 0.7587\n",
      "Epoch: 193, Loss: 0.7586\n",
      "Epoch: 194, Loss: 0.7586\n",
      "Epoch: 195, Loss: 0.7585\n",
      "Epoch: 196, Loss: 0.7587\n",
      "Epoch: 197, Loss: 0.7587\n",
      "Epoch: 198, Loss: 0.7587\n",
      "Epoch: 199, Loss: 0.7586\n",
      "Epoch: 200, Loss: 0.7587\n",
      "Epoch: 201, Loss: 0.7587\n",
      "Epoch: 202, Loss: 0.7587\n",
      "Epoch: 203, Loss: 0.7585\n",
      "Epoch: 204, Loss: 0.7586\n",
      "Epoch: 205, Loss: 0.7584\n",
      "Epoch: 206, Loss: 0.7588\n",
      "Epoch: 207, Loss: 0.7584\n",
      "Epoch: 208, Loss: 0.7585\n",
      "Epoch: 209, Loss: 0.7583\n",
      "Epoch: 210, Loss: 0.7584\n",
      "Epoch: 211, Loss: 0.7585\n",
      "Epoch: 212, Loss: 0.7583\n",
      "Epoch: 213, Loss: 0.7583\n",
      "Epoch: 214, Loss: 0.7583\n",
      "Epoch: 215, Loss: 0.7583\n",
      "Epoch: 216, Loss: 0.7586\n",
      "Epoch: 217, Loss: 0.7583\n",
      "Epoch: 218, Loss: 0.7581\n",
      "Epoch: 219, Loss: 0.7583\n",
      "Epoch: 220, Loss: 0.7582\n",
      "Epoch: 221, Loss: 0.7584\n",
      "Epoch: 222, Loss: 0.7582\n",
      "Epoch: 223, Loss: 0.7582\n",
      "Epoch: 224, Loss: 0.7582\n",
      "Epoch: 225, Loss: 0.7583\n",
      "Epoch: 226, Loss: 0.7582\n",
      "Epoch: 227, Loss: 0.7581\n",
      "Epoch: 228, Loss: 0.7582\n",
      "Epoch: 229, Loss: 0.7581\n",
      "Epoch: 230, Loss: 0.7581\n",
      "Epoch: 231, Loss: 0.7580\n",
      "Epoch: 232, Loss: 0.7581\n",
      "Epoch: 233, Loss: 0.7582\n",
      "Epoch: 234, Loss: 0.7582\n",
      "Epoch: 235, Loss: 0.7579\n",
      "Epoch: 236, Loss: 0.7579\n",
      "Epoch: 237, Loss: 0.7580\n",
      "Epoch: 238, Loss: 0.7577\n",
      "Epoch: 239, Loss: 0.7580\n",
      "Epoch: 240, Loss: 0.7580\n",
      "Epoch: 241, Loss: 0.7580\n",
      "Epoch: 242, Loss: 0.7580\n",
      "Epoch: 243, Loss: 0.7580\n",
      "Epoch: 244, Loss: 0.7580\n",
      "Epoch: 245, Loss: 0.7579\n",
      "Epoch: 246, Loss: 0.7580\n",
      "Epoch: 247, Loss: 0.7577\n",
      "Epoch: 248, Loss: 0.7577\n",
      "Epoch: 249, Loss: 0.7578\n",
      "Epoch: 250, Loss: 0.7576\n",
      "Epoch: 251, Loss: 0.7580\n",
      "Epoch: 252, Loss: 0.7579\n",
      "Epoch: 253, Loss: 0.7580\n",
      "Epoch: 254, Loss: 0.7577\n",
      "Epoch: 255, Loss: 0.7580\n",
      "Epoch: 256, Loss: 0.7576\n",
      "Epoch: 257, Loss: 0.7577\n",
      "Epoch: 258, Loss: 0.7577\n",
      "Epoch: 259, Loss: 0.7579\n",
      "Epoch: 260, Loss: 0.7577\n",
      "Epoch: 261, Loss: 0.7577\n",
      "Epoch: 262, Loss: 0.7577\n",
      "Epoch: 263, Loss: 0.7576\n",
      "Epoch: 264, Loss: 0.7577\n",
      "Epoch: 265, Loss: 0.7579\n",
      "Epoch: 266, Loss: 0.7578\n",
      "Epoch: 267, Loss: 0.7576\n",
      "Epoch: 268, Loss: 0.7576\n",
      "Epoch: 269, Loss: 0.7574\n",
      "Epoch: 270, Loss: 0.7573\n",
      "Epoch: 271, Loss: 0.7576\n",
      "Epoch: 272, Loss: 0.7576\n",
      "Epoch: 273, Loss: 0.7577\n",
      "Epoch: 274, Loss: 0.7575\n",
      "Epoch: 275, Loss: 0.7575\n",
      "Epoch: 276, Loss: 0.7576\n",
      "Epoch: 277, Loss: 0.7574\n",
      "Epoch: 278, Loss: 0.7576\n",
      "Epoch: 279, Loss: 0.7571\n",
      "Epoch: 280, Loss: 0.7573\n",
      "Epoch: 281, Loss: 0.7575\n",
      "Epoch: 282, Loss: 0.7574\n",
      "Epoch: 283, Loss: 0.7575\n",
      "Epoch: 284, Loss: 0.7574\n",
      "Epoch: 285, Loss: 0.7572\n",
      "Epoch: 286, Loss: 0.7572\n",
      "Epoch: 287, Loss: 0.7573\n",
      "Epoch: 288, Loss: 0.7574\n",
      "Epoch: 289, Loss: 0.7574\n",
      "Epoch: 290, Loss: 0.7573\n",
      "Epoch: 291, Loss: 0.7573\n",
      "Epoch: 292, Loss: 0.7574\n",
      "Epoch: 293, Loss: 0.7574\n",
      "Epoch: 294, Loss: 0.7571\n",
      "Epoch: 295, Loss: 0.7574\n",
      "Epoch: 296, Loss: 0.7571\n",
      "Epoch: 297, Loss: 0.7572\n",
      "Epoch: 298, Loss: 0.7572\n",
      "Epoch: 299, Loss: 0.7572\n",
      "Epoch: 300, Loss: 0.7573\n",
      "Epoch: 301, Loss: 0.7571\n",
      "Epoch: 302, Loss: 0.7569\n",
      "Epoch: 303, Loss: 0.7571\n",
      "Epoch: 304, Loss: 0.7572\n",
      "Epoch: 305, Loss: 0.7568\n",
      "Epoch: 306, Loss: 0.7571\n",
      "Epoch: 307, Loss: 0.7572\n",
      "Epoch: 308, Loss: 0.7571\n",
      "Epoch: 309, Loss: 0.7573\n",
      "Epoch: 310, Loss: 0.7571\n",
      "Epoch: 311, Loss: 0.7571\n",
      "Epoch: 312, Loss: 0.7571\n",
      "Epoch: 313, Loss: 0.7572\n",
      "Epoch: 314, Loss: 0.7572\n",
      "Epoch: 315, Loss: 0.7572\n",
      "Epoch: 316, Loss: 0.7572\n",
      "Epoch: 317, Loss: 0.7570\n",
      "Epoch: 318, Loss: 0.7568\n",
      "Epoch: 319, Loss: 0.7569\n",
      "Epoch: 320, Loss: 0.7572\n",
      "Epoch: 321, Loss: 0.7568\n",
      "Epoch: 322, Loss: 0.7570\n",
      "Epoch: 323, Loss: 0.7567\n",
      "Epoch: 324, Loss: 0.7569\n",
      "Epoch: 325, Loss: 0.7568\n",
      "Epoch: 326, Loss: 0.7568\n",
      "Epoch: 327, Loss: 0.7568\n",
      "Epoch: 328, Loss: 0.7569\n",
      "Epoch: 329, Loss: 0.7568\n",
      "Epoch: 330, Loss: 0.7568\n",
      "Epoch: 331, Loss: 0.7569\n",
      "Epoch: 332, Loss: 0.7568\n",
      "Epoch: 333, Loss: 0.7566\n",
      "Epoch: 334, Loss: 0.7568\n",
      "Epoch: 335, Loss: 0.7565\n",
      "Epoch: 336, Loss: 0.7568\n",
      "Epoch: 337, Loss: 0.7568\n",
      "Epoch: 338, Loss: 0.7567\n",
      "Epoch: 339, Loss: 0.7567\n",
      "Epoch: 340, Loss: 0.7568\n",
      "Epoch: 341, Loss: 0.7566\n",
      "Epoch: 342, Loss: 0.7568\n",
      "Epoch: 343, Loss: 0.7568\n",
      "Epoch: 344, Loss: 0.7569\n",
      "Epoch: 345, Loss: 0.7566\n",
      "Epoch: 346, Loss: 0.7565\n",
      "Epoch: 347, Loss: 0.7565\n",
      "Epoch: 348, Loss: 0.7564\n",
      "Epoch: 349, Loss: 0.7566\n",
      "Epoch: 350, Loss: 0.7567\n",
      "Epoch: 351, Loss: 0.7565\n",
      "Epoch: 352, Loss: 0.7566\n",
      "Epoch: 353, Loss: 0.7565\n",
      "Epoch: 354, Loss: 0.7562\n",
      "Epoch: 355, Loss: 0.7563\n",
      "Epoch: 356, Loss: 0.7567\n",
      "Epoch: 357, Loss: 0.7565\n",
      "Epoch: 358, Loss: 0.7567\n",
      "Epoch: 359, Loss: 0.7565\n",
      "Epoch: 360, Loss: 0.7566\n",
      "Epoch: 361, Loss: 0.7563\n",
      "Epoch: 362, Loss: 0.7563\n",
      "Epoch: 363, Loss: 0.7565\n",
      "Epoch: 364, Loss: 0.7563\n",
      "Epoch: 365, Loss: 0.7567\n",
      "Epoch: 366, Loss: 0.7564\n",
      "Epoch: 367, Loss: 0.7564\n",
      "Epoch: 368, Loss: 0.7565\n",
      "Epoch: 369, Loss: 0.7563\n",
      "Epoch: 370, Loss: 0.7565\n",
      "Epoch: 371, Loss: 0.7564\n",
      "Epoch: 372, Loss: 0.7563\n",
      "Epoch: 373, Loss: 0.7562\n",
      "Epoch: 374, Loss: 0.7564\n",
      "Epoch: 375, Loss: 0.7563\n",
      "Epoch: 376, Loss: 0.7561\n",
      "Epoch: 377, Loss: 0.7561\n",
      "Epoch: 378, Loss: 0.7562\n",
      "Epoch: 379, Loss: 0.7563\n",
      "Epoch: 380, Loss: 0.7561\n",
      "Epoch: 381, Loss: 0.7561\n",
      "Epoch: 382, Loss: 0.7563\n",
      "Epoch: 383, Loss: 0.7561\n",
      "Epoch: 384, Loss: 0.7559\n",
      "Epoch: 385, Loss: 0.7563\n",
      "Epoch: 386, Loss: 0.7562\n",
      "Epoch: 387, Loss: 0.7563\n",
      "Epoch: 388, Loss: 0.7563\n",
      "Epoch: 389, Loss: 0.7560\n",
      "Epoch: 390, Loss: 0.7560\n",
      "Epoch: 391, Loss: 0.7561\n",
      "Epoch: 392, Loss: 0.7562\n",
      "Epoch: 393, Loss: 0.7559\n",
      "Epoch: 394, Loss: 0.7561\n",
      "Epoch: 395, Loss: 0.7560\n",
      "Epoch: 396, Loss: 0.7561\n",
      "Epoch: 397, Loss: 0.7558\n",
      "Epoch: 398, Loss: 0.7560\n",
      "Epoch: 399, Loss: 0.7561\n",
      "Epoch: 400, Loss: 0.7559\n",
      "Epoch: 401, Loss: 0.7558\n",
      "Epoch: 402, Loss: 0.7559\n",
      "Epoch: 403, Loss: 0.7559\n",
      "Epoch: 404, Loss: 0.7560\n",
      "Epoch: 405, Loss: 0.7559\n",
      "Epoch: 406, Loss: 0.7557\n",
      "Epoch: 407, Loss: 0.7558\n",
      "Epoch: 408, Loss: 0.7557\n",
      "Epoch: 409, Loss: 0.7559\n",
      "Epoch: 410, Loss: 0.7555\n",
      "Epoch: 411, Loss: 0.7556\n",
      "Epoch: 412, Loss: 0.7557\n",
      "Epoch: 413, Loss: 0.7558\n",
      "Epoch: 414, Loss: 0.7557\n",
      "Epoch: 415, Loss: 0.7558\n",
      "Epoch: 416, Loss: 0.7558\n",
      "Epoch: 417, Loss: 0.7557\n",
      "Epoch: 418, Loss: 0.7557\n",
      "Epoch: 419, Loss: 0.7556\n",
      "Epoch: 420, Loss: 0.7557\n",
      "Epoch: 421, Loss: 0.7555\n",
      "Epoch: 422, Loss: 0.7557\n",
      "Epoch: 423, Loss: 0.7557\n",
      "Epoch: 424, Loss: 0.7558\n",
      "Epoch: 425, Loss: 0.7555\n",
      "Epoch: 426, Loss: 0.7556\n",
      "Epoch: 427, Loss: 0.7555\n",
      "Epoch: 428, Loss: 0.7556\n",
      "Epoch: 429, Loss: 0.7554\n",
      "Epoch: 430, Loss: 0.7555\n",
      "Epoch: 431, Loss: 0.7557\n",
      "Epoch: 432, Loss: 0.7555\n",
      "Epoch: 433, Loss: 0.7555\n",
      "Epoch: 434, Loss: 0.7553\n",
      "Epoch: 435, Loss: 0.7554\n",
      "Epoch: 436, Loss: 0.7552\n",
      "Epoch: 437, Loss: 0.7555\n",
      "Epoch: 438, Loss: 0.7553\n",
      "Epoch: 439, Loss: 0.7553\n",
      "Epoch: 440, Loss: 0.7555\n",
      "Epoch: 441, Loss: 0.7551\n",
      "Epoch: 442, Loss: 0.7553\n",
      "Epoch: 443, Loss: 0.7553\n",
      "Epoch: 444, Loss: 0.7552\n",
      "Epoch: 445, Loss: 0.7555\n",
      "Epoch: 446, Loss: 0.7552\n",
      "Epoch: 447, Loss: 0.7553\n",
      "Epoch: 448, Loss: 0.7552\n",
      "Epoch: 449, Loss: 0.7552\n",
      "Epoch: 450, Loss: 0.7550\n",
      "Epoch: 451, Loss: 0.7551\n",
      "Epoch: 452, Loss: 0.7552\n",
      "Epoch: 453, Loss: 0.7550\n",
      "Epoch: 454, Loss: 0.7551\n",
      "Epoch: 455, Loss: 0.7552\n",
      "Epoch: 456, Loss: 0.7549\n",
      "Epoch: 457, Loss: 0.7552\n",
      "Epoch: 458, Loss: 0.7551\n",
      "Epoch: 459, Loss: 0.7549\n",
      "Epoch: 460, Loss: 0.7548\n",
      "Epoch: 461, Loss: 0.7547\n",
      "Epoch: 462, Loss: 0.7550\n",
      "Epoch: 463, Loss: 0.7549\n",
      "Epoch: 464, Loss: 0.7551\n",
      "Epoch: 465, Loss: 0.7549\n",
      "Epoch: 466, Loss: 0.7549\n",
      "Epoch: 467, Loss: 0.7549\n",
      "Epoch: 468, Loss: 0.7549\n",
      "Epoch: 469, Loss: 0.7549\n",
      "Epoch: 470, Loss: 0.7549\n",
      "Epoch: 471, Loss: 0.7549\n",
      "Epoch: 472, Loss: 0.7547\n",
      "Epoch: 473, Loss: 0.7548\n",
      "Epoch: 474, Loss: 0.7548\n",
      "Epoch: 475, Loss: 0.7547\n",
      "Epoch: 476, Loss: 0.7547\n",
      "Epoch: 477, Loss: 0.7547\n",
      "Epoch: 478, Loss: 0.7548\n",
      "Epoch: 479, Loss: 0.7548\n",
      "Epoch: 480, Loss: 0.7548\n",
      "Epoch: 481, Loss: 0.7546\n",
      "Epoch: 482, Loss: 0.7546\n",
      "Epoch: 483, Loss: 0.7545\n",
      "Epoch: 484, Loss: 0.7546\n",
      "Epoch: 485, Loss: 0.7543\n",
      "Epoch: 486, Loss: 0.7545\n",
      "Epoch: 487, Loss: 0.7549\n",
      "Epoch: 488, Loss: 0.7544\n",
      "Epoch: 489, Loss: 0.7545\n",
      "Epoch: 490, Loss: 0.7545\n",
      "Epoch: 491, Loss: 0.7546\n",
      "Epoch: 492, Loss: 0.7545\n",
      "Epoch: 493, Loss: 0.7542\n",
      "Epoch: 494, Loss: 0.7544\n",
      "Epoch: 495, Loss: 0.7546\n",
      "Epoch: 496, Loss: 0.7543\n",
      "Epoch: 497, Loss: 0.7545\n",
      "Epoch: 498, Loss: 0.7540\n",
      "Epoch: 499, Loss: 0.7545\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:11:57.696907Z",
     "start_time": "2024-04-24T17:11:57.275281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# draw loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ],
   "id": "a390a5cf243d293d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "135c3d5095125b58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:12:04.292883Z",
     "start_time": "2024-04-24T17:12:04.261272Z"
    }
   },
   "source": [
    "go_embedding = model()  # go_index -> go_embedding\n",
    "go_embedding.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9917, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "e724e321c682bfbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:12:06.449989Z",
     "start_time": "2024-04-24T17:12:06.386920Z"
    }
   },
   "source": [
    "_embedding = go_embedding.cpu().detach().numpy()\n",
    "\n",
    "map_int_gene = {int(idx): gene for idx, gene in enumerate(counts1.columns)}\n",
    "map_gene_int = {gene: idx for idx, gene in map_int_gene.items()}\n",
    "\n",
    "# convert to gene_index -> go_embedding\n",
    "embedding_gene = np.random.randn(len(counts1.columns), _embedding.shape[1])\n",
    "for idx, gene in map_int_gene.items():\n",
    "    if gene in map_go_int:\n",
    "        embedding_gene[idx] = _embedding[map_go_int[gene]]\n",
    "\n",
    "embedding_gene = torch.tensor(embedding_gene, dtype=torch.float32, device=device)\n",
    "embedding_gene.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52645, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:12:12.403805Z",
     "start_time": "2024-04-24T17:12:12.372838Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(embedding_gene, '../data/embedding_gene.pt')",
   "id": "30bc7341fb3bbc60",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "93c17bd4cac55149",
   "metadata": {},
   "source": [
    "# process the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e74a2b4749d6ce5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:14:04.929846Z",
     "start_time": "2024-04-24T17:14:04.607777Z"
    }
   },
   "source": [
    "# add age, sex, lithium of pheno1 to counts1\n",
    "tmp_pheno1 = pheno1[[\"age\", \"sex\", \"lithium\"]].apply(lambda x: x.replace(\"M\", 0).replace(\"F\", 1))  # chagne sex to 0, 1\n",
    "counts1_merge = pd.merge(counts1, tmp_pheno1, left_index=True, right_index=True)\n",
    "\n",
    "counts1_merge = (counts1_merge - counts1_merge.mean()) / counts1_merge.std()\n",
    "counts1_merge.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         ENSG00000000003  ENSG00000000005  ENSG00000000419  ENSG00000000457  \\\n",
       "089357B         2.110648         4.704691        -0.899272        -1.436269   \n",
       "089366A         1.348549         1.129061        -0.091576         0.088378   \n",
       "089412B         0.586450        -0.301190         0.955766        -0.256079   \n",
       "089425B         0.840483        -0.301190        -0.615247         0.003676   \n",
       "089687A        -0.429682        -0.301190        -1.023533        -1.289451   \n",
       "\n",
       "         ENSG00000000460  ENSG00000000938  ENSG00000000971  ENSG00000001036  \\\n",
       "089357B        -1.132860        -1.111674        -0.804400        -1.458287   \n",
       "089366A        -0.357019        -0.170067        -0.695273        -0.660205   \n",
       "089412B        -0.438687         0.146277        -0.531583         0.106580   \n",
       "089425B         0.112569        -0.591117        -0.877151        -0.941881   \n",
       "089687A        -1.602448        -1.019593        -0.913527        -1.223557   \n",
       "\n",
       "         ENSG00000001084  ENSG00000001167  ...  ENSGR0000178605  \\\n",
       "089357B        -0.629450        -1.040200  ...        -0.152399   \n",
       "089366A        -0.785903        -0.267949  ...        -0.152399   \n",
       "089412B         0.249663         0.314793  ...        -0.152399   \n",
       "089425B        -0.815703         0.158447  ...        -0.152399   \n",
       "089687A        -1.262710        -1.532924  ...        -0.152399   \n",
       "\n",
       "         ENSGR0000182378  ENSGR0000185291  ENSGR0000198223  ENSGR0000214717  \\\n",
       "089357B        -0.453101        -0.212458        -0.502577         -0.45051   \n",
       "089366A        -0.453101        -0.212458        -0.146685         -0.45051   \n",
       "089412B        -0.453101        -0.212458        -0.146685         -0.45051   \n",
       "089425B        -0.453101        -0.212458        -0.502577         -0.45051   \n",
       "089687A        -0.453101        -0.212458        -0.146685         -0.45051   \n",
       "\n",
       "         ENSGR0000223511  ENSGR0000223773       age       sex   lithium  \n",
       "089357B        -0.222561        -0.226593 -2.043569  0.879916 -0.720677  \n",
       "089366A        -0.222561         2.492525 -1.972198  0.879916 -0.720677  \n",
       "089412B        -0.222561        -0.226593 -1.686712  0.879916 -0.720677  \n",
       "089425B        -0.222561        -0.226593  0.026202  0.879916 -0.720677  \n",
       "089687A        -0.222561        -0.226593  0.383059  0.879916 -0.720677  \n",
       "\n",
       "[5 rows x 52648 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000000003</th>\n",
       "      <th>ENSG00000000005</th>\n",
       "      <th>ENSG00000000419</th>\n",
       "      <th>ENSG00000000457</th>\n",
       "      <th>ENSG00000000460</th>\n",
       "      <th>ENSG00000000938</th>\n",
       "      <th>ENSG00000000971</th>\n",
       "      <th>ENSG00000001036</th>\n",
       "      <th>ENSG00000001084</th>\n",
       "      <th>ENSG00000001167</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSGR0000178605</th>\n",
       "      <th>ENSGR0000182378</th>\n",
       "      <th>ENSGR0000185291</th>\n",
       "      <th>ENSGR0000198223</th>\n",
       "      <th>ENSGR0000214717</th>\n",
       "      <th>ENSGR0000223511</th>\n",
       "      <th>ENSGR0000223773</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>lithium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>089357B</th>\n",
       "      <td>2.110648</td>\n",
       "      <td>4.704691</td>\n",
       "      <td>-0.899272</td>\n",
       "      <td>-1.436269</td>\n",
       "      <td>-1.132860</td>\n",
       "      <td>-1.111674</td>\n",
       "      <td>-0.804400</td>\n",
       "      <td>-1.458287</td>\n",
       "      <td>-0.629450</td>\n",
       "      <td>-1.040200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152399</td>\n",
       "      <td>-0.453101</td>\n",
       "      <td>-0.212458</td>\n",
       "      <td>-0.502577</td>\n",
       "      <td>-0.45051</td>\n",
       "      <td>-0.222561</td>\n",
       "      <td>-0.226593</td>\n",
       "      <td>-2.043569</td>\n",
       "      <td>0.879916</td>\n",
       "      <td>-0.720677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089366A</th>\n",
       "      <td>1.348549</td>\n",
       "      <td>1.129061</td>\n",
       "      <td>-0.091576</td>\n",
       "      <td>0.088378</td>\n",
       "      <td>-0.357019</td>\n",
       "      <td>-0.170067</td>\n",
       "      <td>-0.695273</td>\n",
       "      <td>-0.660205</td>\n",
       "      <td>-0.785903</td>\n",
       "      <td>-0.267949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152399</td>\n",
       "      <td>-0.453101</td>\n",
       "      <td>-0.212458</td>\n",
       "      <td>-0.146685</td>\n",
       "      <td>-0.45051</td>\n",
       "      <td>-0.222561</td>\n",
       "      <td>2.492525</td>\n",
       "      <td>-1.972198</td>\n",
       "      <td>0.879916</td>\n",
       "      <td>-0.720677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089412B</th>\n",
       "      <td>0.586450</td>\n",
       "      <td>-0.301190</td>\n",
       "      <td>0.955766</td>\n",
       "      <td>-0.256079</td>\n",
       "      <td>-0.438687</td>\n",
       "      <td>0.146277</td>\n",
       "      <td>-0.531583</td>\n",
       "      <td>0.106580</td>\n",
       "      <td>0.249663</td>\n",
       "      <td>0.314793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152399</td>\n",
       "      <td>-0.453101</td>\n",
       "      <td>-0.212458</td>\n",
       "      <td>-0.146685</td>\n",
       "      <td>-0.45051</td>\n",
       "      <td>-0.222561</td>\n",
       "      <td>-0.226593</td>\n",
       "      <td>-1.686712</td>\n",
       "      <td>0.879916</td>\n",
       "      <td>-0.720677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089425B</th>\n",
       "      <td>0.840483</td>\n",
       "      <td>-0.301190</td>\n",
       "      <td>-0.615247</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.112569</td>\n",
       "      <td>-0.591117</td>\n",
       "      <td>-0.877151</td>\n",
       "      <td>-0.941881</td>\n",
       "      <td>-0.815703</td>\n",
       "      <td>0.158447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152399</td>\n",
       "      <td>-0.453101</td>\n",
       "      <td>-0.212458</td>\n",
       "      <td>-0.502577</td>\n",
       "      <td>-0.45051</td>\n",
       "      <td>-0.222561</td>\n",
       "      <td>-0.226593</td>\n",
       "      <td>0.026202</td>\n",
       "      <td>0.879916</td>\n",
       "      <td>-0.720677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089687A</th>\n",
       "      <td>-0.429682</td>\n",
       "      <td>-0.301190</td>\n",
       "      <td>-1.023533</td>\n",
       "      <td>-1.289451</td>\n",
       "      <td>-1.602448</td>\n",
       "      <td>-1.019593</td>\n",
       "      <td>-0.913527</td>\n",
       "      <td>-1.223557</td>\n",
       "      <td>-1.262710</td>\n",
       "      <td>-1.532924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152399</td>\n",
       "      <td>-0.453101</td>\n",
       "      <td>-0.212458</td>\n",
       "      <td>-0.146685</td>\n",
       "      <td>-0.45051</td>\n",
       "      <td>-0.222561</td>\n",
       "      <td>-0.226593</td>\n",
       "      <td>0.383059</td>\n",
       "      <td>0.879916</td>\n",
       "      <td>-0.720677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 52648 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "6352a7375c960f94",
   "metadata": {},
   "source": [
    "bp_db_genes = set(bp_db.ENSEMBL)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "7676409a345d9cd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:30:26.219673Z",
     "start_time": "2024-04-24T17:30:17.513017Z"
    }
   },
   "source": [
    "dataset = []\n",
    "input_dim = None\n",
    "\n",
    "for row in counts1_merge.iterrows():\n",
    "    idx = row[0]\n",
    "    values = row[1]\n",
    "    _data = {\n",
    "        \"gene_idx\": [],\n",
    "        \"gene_value\": [],\n",
    "        \"other_info\": []\n",
    "    }\n",
    "    for k, v in values.items():\n",
    "        if k in map_gene_int:\n",
    "            _data[\"gene_idx\"].append(map_gene_int[k])\n",
    "            _data[\"gene_value\"].append(v)\n",
    "        else:\n",
    "            _data[\"other_info\"].append(v)\n",
    "    dataset.append(_data)\n",
    "    if input_dim is None:\n",
    "        input_dim = [len(_data[\"gene_idx\"]), len(_data[\"gene_value\"]), len(_data[\"other_info\"])]\n",
    "\n",
    "input_dim"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52645, 52645, 3]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "6db3f7cf05dbade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:23:25.676975Z",
     "start_time": "2024-04-24T17:23:25.659196Z"
    }
   },
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx]\n",
    "        gene_idx = torch.tensor(sample['gene_idx'], dtype=torch.long, device=device)\n",
    "        gene_value = torch.tensor(sample['gene_value'], dtype=torch.float, device=device)\n",
    "        other_info = torch.tensor(sample['other_info'], dtype=torch.float, device=device)\n",
    "\n",
    "        return {\n",
    "            'gene_idx': gene_idx,\n",
    "            'gene_value': gene_value,\n",
    "            'other_info': other_info\n",
    "        }, torch.tensor(self.y[idx], dtype=torch.long, device=device)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "7138ab82252b3773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:28:42.554086Z",
     "start_time": "2024-04-24T17:28:42.546496Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset\n",
    "y = pheno1[\"condition\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(len(X_train), len(X_test))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310 134\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "84130e64e014ae3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:28:42.729123Z",
     "start_time": "2024-04-24T17:28:42.588124Z"
    }
   },
   "source": [
    "train_dataset = MyDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "test_dataset = MyDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DNN Classifier with Gene Embedding",
   "id": "adefc2c87b795b20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:36:50.174893Z",
     "start_time": "2024-04-24T17:36:50.166304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, node_embedding, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = node_embedding\n",
    "        #self.embedding.requires_grad = False\n",
    "        self.fc0_1 = nn.Linear(9, 128)\n",
    "        self.fc0_2 = nn.Linear(128, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(sum(input_dim), 512)\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.05)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        gene_idx = X['gene_idx']  # (B, N_1)\n",
    "        gene_value = X['gene_value']  # (B, N_1)\n",
    "        other_info = X['other_info']  # (B, N_2)\n",
    "\n",
    "        gene_embedding = self.embedding[gene_idx]  # (B, N_1, 8)\n",
    "        gene_embedding = torch.concat([gene_embedding, gene_value.unsqueeze(2)], dim=2)  # (B, N_1, 9)\n",
    "        gene_embedding = self.fc0_1(gene_embedding)  # (B, N_1, 128)\n",
    "        gene_embedding = self.relu(gene_embedding)\n",
    "        gene_embedding = self.fc0_2(gene_embedding).squeeze(2)  # (B, N_1)\n",
    "\n",
    "        output = torch.cat([gene_embedding, gene_value, other_info], dim=1)  # (B, N_1 * 2 + N_3)\n",
    "\n",
    "        output = self.fc1(output)  # (B, 512)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout1(output)\n",
    "\n",
    "        output = self.fc2(output)  # (B, 128)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout2(output)\n",
    "\n",
    "        return self.fc3(output)  # (B, 2)"
   ],
   "id": "8aae4c7930920bca",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:36:50.253941Z",
     "start_time": "2024-04-24T17:36:50.239943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    total_correct = 0\n",
    "    total_instances = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for X, y in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Assuming your model's forward method automatically handles padding, then no need to pack sequence here\n",
    "        predictions = model(X)\n",
    "\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Compute the number of correct predictions\n",
    "        _, predicted_classes = predictions.max(dim=1)\n",
    "        correct_predictions = (predicted_classes == y).float()  # Convert to float for summation\n",
    "        total_correct += correct_predictions.sum().item()\n",
    "        total_instances += y.size(0)\n",
    "\n",
    "    epoch_acc = total_correct / total_instances\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc\n",
    "\n",
    "#train_loss, train_acc = train(model, train_loader, optimizer, criterion)"
   ],
   "id": "399cda7df431190b",
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "21b4baa715ce7bd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:36:50.269941Z",
     "start_time": "2024-04-24T17:36:50.254942Z"
    }
   },
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    total_correct = 0\n",
    "    total_instances = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in iterator:\n",
    "            predictions = model(X)\n",
    "\n",
    "            loss = criterion(predictions, y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Compute the number of correct predictions\n",
    "            _, predicted_classes = predictions.max(dim=1)\n",
    "            correct_predictions = (predicted_classes == y).float()  # Convert to float for summation\n",
    "            total_correct += correct_predictions.sum().item()\n",
    "            total_instances += y.size(0)\n",
    "\n",
    "    epoch_acc = total_correct / total_instances\n",
    "    return epoch_loss / len(iterator), epoch_acc"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "3c7554b7e500ba06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:36:50.285941Z",
     "start_time": "2024-04-24T17:36:50.270943Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = elapsed_time - (elapsed_mins * 60)\n",
    "    return elapsed_mins, elapsed_secs, elapsed_time"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "ec76852d146ec0ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:37:50.720697Z",
     "start_time": "2024-04-24T17:36:50.286943Z"
    }
   },
   "source": [
    "embedding_gene = torch.load('../data/embedding_gene.pt')\n",
    "model = MyModel(node_embedding=embedding_gene, input_dim=input_dim, output_dim=2)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "model = model.to(device)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "best_valid_acc = 0\n",
    "\n",
    "elapsed_times = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, test_loader, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs, elapsed_time = epoch_time(start_time, end_time)\n",
    "    elapsed_times.append(elapsed_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        #torch.save(model.state_dict(), '../data/final_model.pt')\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs:.3f}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
    "\n",
    "avg_elapsed_time = sum(elapsed_times) / len(elapsed_times)\n",
    "print(f'Avg Epoch Time: {avg_elapsed_time:.3f}s')\n",
    "print(f'Best Val. Loss: {best_valid_loss:.3f} | Best Val. Acc: {best_valid_acc * 100:.2f}%')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 3.011s\n",
      "\tTrain Loss: 0.671 | Train Acc: 62.26%\n",
      "\t Val. Loss: 0.675 |  Val. Acc: 55.22%\n",
      "Epoch: 02 | Epoch Time: 0m 2.999s\n",
      "\tTrain Loss: 0.559 | Train Acc: 79.03%\n",
      "\t Val. Loss: 0.678 |  Val. Acc: 57.46%\n",
      "Epoch: 03 | Epoch Time: 0m 2.977s\n",
      "\tTrain Loss: 0.457 | Train Acc: 85.81%\n",
      "\t Val. Loss: 0.593 |  Val. Acc: 68.66%\n",
      "Epoch: 04 | Epoch Time: 0m 3.021s\n",
      "\tTrain Loss: 0.347 | Train Acc: 92.90%\n",
      "\t Val. Loss: 0.602 |  Val. Acc: 67.91%\n",
      "Epoch: 05 | Epoch Time: 0m 3.000s\n",
      "\tTrain Loss: 0.251 | Train Acc: 95.48%\n",
      "\t Val. Loss: 0.529 |  Val. Acc: 71.64%\n",
      "Epoch: 06 | Epoch Time: 0m 3.000s\n",
      "\tTrain Loss: 0.181 | Train Acc: 98.06%\n",
      "\t Val. Loss: 0.547 |  Val. Acc: 70.15%\n",
      "Epoch: 07 | Epoch Time: 0m 2.999s\n",
      "\tTrain Loss: 0.128 | Train Acc: 99.35%\n",
      "\t Val. Loss: 0.569 |  Val. Acc: 70.90%\n",
      "Epoch: 08 | Epoch Time: 0m 3.015s\n",
      "\tTrain Loss: 0.098 | Train Acc: 99.03%\n",
      "\t Val. Loss: 0.517 |  Val. Acc: 69.40%\n",
      "Epoch: 09 | Epoch Time: 0m 3.012s\n",
      "\tTrain Loss: 0.064 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.432 |  Val. Acc: 79.85%\n",
      "Epoch: 10 | Epoch Time: 0m 2.984s\n",
      "\tTrain Loss: 0.053 | Train Acc: 99.68%\n",
      "\t Val. Loss: 0.469 |  Val. Acc: 78.36%\n",
      "Epoch: 11 | Epoch Time: 0m 2.990s\n",
      "\tTrain Loss: 0.040 | Train Acc: 99.68%\n",
      "\t Val. Loss: 0.420 |  Val. Acc: 81.34%\n",
      "Epoch: 12 | Epoch Time: 0m 3.015s\n",
      "\tTrain Loss: 0.035 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.505 |  Val. Acc: 77.61%\n",
      "Epoch: 13 | Epoch Time: 0m 2.999s\n",
      "\tTrain Loss: 0.028 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.435 |  Val. Acc: 80.60%\n",
      "Epoch: 14 | Epoch Time: 0m 3.005s\n",
      "\tTrain Loss: 0.022 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.465 |  Val. Acc: 79.10%\n",
      "Epoch: 15 | Epoch Time: 0m 3.065s\n",
      "\tTrain Loss: 0.022 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.481 |  Val. Acc: 77.61%\n",
      "Epoch: 16 | Epoch Time: 0m 3.045s\n",
      "\tTrain Loss: 0.019 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.424 |  Val. Acc: 82.09%\n",
      "Epoch: 17 | Epoch Time: 0m 3.010s\n",
      "\tTrain Loss: 0.017 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.501 |  Val. Acc: 76.87%\n",
      "Epoch: 18 | Epoch Time: 0m 2.991s\n",
      "\tTrain Loss: 0.015 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.485 |  Val. Acc: 76.87%\n",
      "Epoch: 19 | Epoch Time: 0m 3.040s\n",
      "\tTrain Loss: 0.015 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.430 |  Val. Acc: 82.84%\n",
      "Epoch: 20 | Epoch Time: 0m 3.018s\n",
      "\tTrain Loss: 0.011 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.465 |  Val. Acc: 79.85%\n",
      "Avg Epoch Time: 3.010s\n",
      "Best Val. Loss: 0.420 | Best Val. Acc: 82.84%\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DNN Classifier without Gene Embedding",
   "id": "155fcb573ee47813"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:37:50.736697Z",
     "start_time": "2024-04-24T17:37:50.721698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MyModelBaseline(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(sum(input_dim[1:]), 512)\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.05)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        gene_value = X['gene_value']  # (B, N_1)\n",
    "        other_info = X['other_info']  # (B, N_2)\n",
    "        output = torch.cat([gene_value, other_info], dim=1)  # (B, N_1 + N_3)\n",
    "\n",
    "        output = self.fc1(output)  # (B, 512)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout1(output)\n",
    "\n",
    "        output = self.fc2(output)  # (B, 128)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout2(output)\n",
    "\n",
    "        return self.fc3(output)  # (B, 2)"
   ],
   "id": "4a73737bd0358716",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:38:45.017334Z",
     "start_time": "2024-04-24T17:37:50.737699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = MyModelBaseline(input_dim=input_dim, output_dim=2)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "model = model.to(device)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "best_valid_acc = 0\n",
    "\n",
    "elapsed_times = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, test_loader, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs, elapsed_time = epoch_time(start_time, end_time)\n",
    "    elapsed_times.append(elapsed_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        #torch.save(model.state_dict(), '../data/final_model.pt')\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs:.3f}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
    "\n",
    "avg_elapsed_time = sum(elapsed_times) / len(elapsed_times)\n",
    "print(f'Avg Epoch Time: {avg_elapsed_time:.3f}s')\n",
    "print(f'Best Val. Loss: {best_valid_loss:.3f} | Best Val. Acc: {best_valid_acc * 100:.2f}%')"
   ],
   "id": "2497ed8dd5f3652f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 2.514s\n",
      "\tTrain Loss: 0.671 | Train Acc: 59.03%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 55.22%\n",
      "Epoch: 02 | Epoch Time: 0m 2.586s\n",
      "\tTrain Loss: 0.572 | Train Acc: 75.81%\n",
      "\t Val. Loss: 0.657 |  Val. Acc: 60.45%\n",
      "Epoch: 03 | Epoch Time: 0m 2.647s\n",
      "\tTrain Loss: 0.485 | Train Acc: 86.77%\n",
      "\t Val. Loss: 0.639 |  Val. Acc: 63.43%\n",
      "Epoch: 04 | Epoch Time: 0m 2.715s\n",
      "\tTrain Loss: 0.408 | Train Acc: 91.29%\n",
      "\t Val. Loss: 0.614 |  Val. Acc: 67.16%\n",
      "Epoch: 05 | Epoch Time: 0m 2.735s\n",
      "\tTrain Loss: 0.330 | Train Acc: 94.84%\n",
      "\t Val. Loss: 0.570 |  Val. Acc: 70.90%\n",
      "Epoch: 06 | Epoch Time: 0m 2.752s\n",
      "\tTrain Loss: 0.267 | Train Acc: 96.77%\n",
      "\t Val. Loss: 0.547 |  Val. Acc: 72.39%\n",
      "Epoch: 07 | Epoch Time: 0m 2.708s\n",
      "\tTrain Loss: 0.207 | Train Acc: 98.71%\n",
      "\t Val. Loss: 0.560 |  Val. Acc: 72.39%\n",
      "Epoch: 08 | Epoch Time: 0m 2.710s\n",
      "\tTrain Loss: 0.160 | Train Acc: 98.71%\n",
      "\t Val. Loss: 0.526 |  Val. Acc: 76.12%\n",
      "Epoch: 09 | Epoch Time: 0m 2.712s\n",
      "\tTrain Loss: 0.126 | Train Acc: 99.35%\n",
      "\t Val. Loss: 0.500 |  Val. Acc: 75.37%\n",
      "Epoch: 10 | Epoch Time: 0m 2.706s\n",
      "\tTrain Loss: 0.099 | Train Acc: 99.68%\n",
      "\t Val. Loss: 0.534 |  Val. Acc: 74.63%\n",
      "Epoch: 11 | Epoch Time: 0m 2.734s\n",
      "\tTrain Loss: 0.078 | Train Acc: 99.68%\n",
      "\t Val. Loss: 0.491 |  Val. Acc: 75.37%\n",
      "Epoch: 12 | Epoch Time: 0m 2.739s\n",
      "\tTrain Loss: 0.065 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.518 |  Val. Acc: 74.63%\n",
      "Epoch: 13 | Epoch Time: 0m 2.751s\n",
      "\tTrain Loss: 0.052 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.514 |  Val. Acc: 75.37%\n",
      "Epoch: 14 | Epoch Time: 0m 2.709s\n",
      "\tTrain Loss: 0.044 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.502 |  Val. Acc: 74.63%\n",
      "Epoch: 15 | Epoch Time: 0m 2.712s\n",
      "\tTrain Loss: 0.035 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.531 |  Val. Acc: 75.37%\n",
      "Epoch: 16 | Epoch Time: 0m 2.735s\n",
      "\tTrain Loss: 0.035 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.459 |  Val. Acc: 74.63%\n",
      "Epoch: 17 | Epoch Time: 0m 2.756s\n",
      "\tTrain Loss: 0.030 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.520 |  Val. Acc: 76.87%\n",
      "Epoch: 18 | Epoch Time: 0m 2.757s\n",
      "\tTrain Loss: 0.025 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.514 |  Val. Acc: 76.87%\n",
      "Epoch: 19 | Epoch Time: 0m 2.733s\n",
      "\tTrain Loss: 0.023 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.498 |  Val. Acc: 76.87%\n",
      "Epoch: 20 | Epoch Time: 0m 2.730s\n",
      "\tTrain Loss: 0.022 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.502 |  Val. Acc: 76.12%\n",
      "Avg Epoch Time: 2.707s\n",
      "Best Val. Loss: 0.459 | Best Val. Acc: 76.87%\n"
     ]
    }
   ],
   "execution_count": 55
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
